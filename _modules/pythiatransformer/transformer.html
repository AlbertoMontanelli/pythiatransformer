<!DOCTYPE html>
<html lang="en"
      data-content_root="../../"
      x-data="{ darkMode: localStorage.getItem('darkMode') || localStorage.setItem('darkMode', 'system'), activeSection: '' }"
      x-init="$watch('darkMode', val => localStorage.setItem('darkMode', val))"
      class="scroll-smooth"
      :class="{'dark': darkMode === 'dark' || (darkMode === 'system' && window.matchMedia('(prefers-color-scheme: dark)').matches)}"
>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta charset="utf-8" />
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="white" />
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="black" />
  
    <title>pythiatransformer.transformer | pythiatransformer 0.1.0 documentation</title>
    <meta property="og:title" content="pythiatransformer.transformer | pythiatransformer 0.1.0 documentation" />
    <meta name="twitter:title" content="pythiatransformer.transformer | pythiatransformer 0.1.0 documentation" />
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/theme.css?v=42baaae4" />
        <link rel="search" title="Search" href="../../search.html" />
        <link rel="index" title="Index" href="../../genindex.html" />

    <script>
    <!-- Prevent Flash of wrong theme -->
      const userPreference = localStorage.getItem('darkMode');
      let mode;
      if (userPreference === 'dark' || window.matchMedia('(prefers-color-scheme: dark)').matches) {
        mode = 'dark';
        document.documentElement.classList.add('dark');
      } else {
        mode = 'light';
      }
      if (!userPreference) {localStorage.setItem('darkMode', mode)}
    </script>
</head>
  <body x-data="{ showSidebar: false, showScrollTop: false }" class="min-h-screen font-sans antialiased bg-background text-foreground" :class="{ 'overflow-hidden': showSidebar }">
    <div x-cloak x-show="showSidebar" class="fixed inset-0 z-50 overflow-hidden bg-background/80 backdrop-blur-sm md:hidden" @click.self="showSidebar = false"></div><div id="page" class="relative flex flex-col min-h-screen"><a href="#content" class="absolute top-0 left-0 z-[100] block bg-background p-4 text-xl transition -translate-x-full opacity-0 focus:translate-x-0 focus:opacity-100">
      Skip to content
    </a><header
  class="sticky top-0 z-40 w-full border-b shadow-sm border-border supports-backdrop-blur:bg-background/60 bg-background/95 backdrop-blur"><div class="container flex items-center h-14">
    <div class="hidden mr-4 md:flex">
      <a href="../../index.html" class="flex items-center mr-6"><span class="hidden font-bold sm:inline-block text-clip whitespace-nowrap">pythiatransformer 0.1.0 documentation</span>
      </a></div><button
      class="inline-flex items-center justify-center h-10 px-0 py-2 mr-2 text-base font-medium transition-colors rounded-md hover:text-accent-foreground hover:bg-transparent md:hidden"
      type="button" @click="showSidebar = true">
      <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" aria-hidden="true"
        fill="currentColor">
        <path
          d="M152.587 825.087q-19.152 0-32.326-13.174t-13.174-32.326q0-19.152 13.174-32.326t32.326-13.174h440q19.152 0 32.326 13.174t13.174 32.326q0 19.152-13.174 32.326t-32.326 13.174h-440Zm0-203.587q-19.152 0-32.326-13.174T107.087 576q0-19.152 13.174-32.326t32.326-13.174h320q19.152 0 32.326 13.174T518.087 576q0 19.152-13.174 32.326T472.587 621.5h-320Zm0-203.587q-19.152 0-32.326-13.174t-13.174-32.326q0-19.152 13.174-32.326t32.326-13.174h440q19.152 0 32.326 13.174t13.174 32.326q0 19.152-13.174 32.326t-32.326 13.174h-440ZM708.913 576l112.174 112.174q12.674 12.674 12.674 31.826t-12.674 31.826Q808.413 764.5 789.261 764.5t-31.826-12.674l-144-144Q600 594.391 600 576t13.435-31.826l144-144q12.674-12.674 31.826-12.674t31.826 12.674q12.674 12.674 12.674 31.826t-12.674 31.826L708.913 576Z" />
      </svg>
      <span class="sr-only">Toggle navigation menu</span>
    </button>
    <div class="flex items-center justify-between flex-1 space-x-2 sm:space-x-4 md:justify-end">
      <div class="flex-1 w-full md:w-auto md:flex-none"><form id="searchbox"
      action="../../search.html"
      method="get"
      class="relative flex items-center group"
      @keydown.k.window.meta="$refs.search.focus()">
  <input x-ref="search"
          name="q"
          id="search-input"
          type="search"
          aria-label="Search the docs"
          placeholder="Search ..."
          class="inline-flex items-center font-medium transition-colors bg-transparent focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 ring-offset-background border border-input hover:bg-accent focus:bg-accent hover:text-accent-foreground focus:text-accent-foreground hover:placeholder-accent-foreground py-2 px-4 relative h-9 w-full justify-start rounded-[0.5rem] text-sm text-muted-foreground sm:pr-12 md:w-40 lg:w-64" />
  <kbd class="pointer-events-none absolute right-1.5 top-2 hidden h-5 select-none text-muted-foreground items-center gap-1 rounded border border-border bg-muted px-1.5 font-mono text-[10px] font-medium opacity-100 sm:flex group-hover:bg-accent group-hover:text-accent-foreground">
    <span class="text-xs">⌘</span>
    K
  </kbd>
</form>
      </div>
      <nav class="flex items-center space-x-1">
        <button @click="darkMode = darkMode === 'light' ? 'dark' : 'light'"
          class="relative inline-flex items-center justify-center px-0 text-sm font-medium transition-colors rounded-md hover:bg-accent hover:text-accent-foreground h-9 w-9"
          type="button"
          aria-label="Color theme switcher">
          <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" fill="currentColor"
            class="absolute transition-all scale-100 rotate-0 dark:-rotate-90 dark:scale-0">
            <path
              d="M480 685q45.456 0 77.228-31.772Q589 621.456 589 576q0-45.456-31.772-77.228Q525.456 467 480 467q-45.456 0-77.228 31.772Q371 530.544 371 576q0 45.456 31.772 77.228Q434.544 685 480 685Zm0 91q-83 0-141.5-58.5T280 576q0-83 58.5-141.5T480 376q83 0 141.5 58.5T680 576q0 83-58.5 141.5T480 776ZM80 621.5q-19.152 0-32.326-13.174T34.5 576q0-19.152 13.174-32.326T80 530.5h80q19.152 0 32.326 13.174T205.5 576q0 19.152-13.174 32.326T160 621.5H80Zm720 0q-19.152 0-32.326-13.174T754.5 576q0-19.152 13.174-32.326T800 530.5h80q19.152 0 32.326 13.174T925.5 576q0 19.152-13.174 32.326T880 621.5h-80Zm-320-320q-19.152 0-32.326-13.174T434.5 256v-80q0-19.152 13.174-32.326T480 130.5q19.152 0 32.326 13.174T525.5 176v80q0 19.152-13.174 32.326T480 301.5Zm0 720q-19.152 0-32.326-13.17Q434.5 995.152 434.5 976v-80q0-19.152 13.174-32.326T480 850.5q19.152 0 32.326 13.174T525.5 896v80q0 19.152-13.174 32.33-13.174 13.17-32.326 13.17ZM222.174 382.065l-43-42Q165.5 327.391 166 308.239t13.174-33.065q13.435-13.674 32.587-13.674t32.065 13.674l42.239 43q12.674 13.435 12.555 31.706-.12 18.272-12.555 31.946-12.674 13.674-31.445 13.413-18.772-.261-32.446-13.174Zm494 494.761-42.239-43q-12.674-13.435-12.674-32.087t12.674-31.565Q686.609 756.5 705.38 757q18.772.5 32.446 13.174l43 41.761Q794.5 824.609 794 843.761t-13.174 33.065Q767.391 890.5 748.239 890.5t-32.065-13.674Zm-42-494.761Q660.5 369.391 661 350.62q.5-18.772 13.174-32.446l41.761-43Q728.609 261.5 747.761 262t33.065 13.174q13.674 13.435 13.674 32.587t-13.674 32.065l-43 42.239q-13.435 12.674-31.706 12.555-18.272-.12-31.946-12.555Zm-495 494.761Q165.5 863.391 165.5 844.239t13.674-32.065l43-42.239q13.435-12.674 32.087-12.674t31.565 12.674Q299.5 782.609 299 801.38q-.5 18.772-13.174 32.446l-41.761 43Q231.391 890.5 212.239 890t-33.065-13.174ZM480 576Z" />
          </svg>
          <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" fill="currentColor"
            class="absolute transition-all scale-0 rotate-90 dark:rotate-0 dark:scale-100">
            <path
              d="M480 936q-151 0-255.5-104.5T120 576q0-138 90-239.5T440 218q25-3 39 18t-1 44q-17 26-25.5 55t-8.5 61q0 90 63 153t153 63q31 0 61.5-9t54.5-25q21-14 43-1.5t19 39.5q-14 138-117.5 229T480 936Zm0-80q88 0 158-48.5T740 681q-20 5-40 8t-40 3q-123 0-209.5-86.5T364 396q0-20 3-40t8-40q-78 32-126.5 102T200 576q0 116 82 198t198 82Zm-10-270Z" />
          </svg>
        </button>
      </nav>
    </div>
  </div>
</header>

    <div class="flex-1"><div class="container flex-1 items-start md:grid md:grid-cols-[220px_minmax(0,1fr)] md:gap-6 lg:grid-cols-[240px_minmax(0,1fr)] lg:gap-10"><aside id="left-sidebar"
  class="fixed inset-y-0 left-0 md:top-14 z-50 md:z-30 bg-background md:bg-transparent transition-all duration-100 -translate-x-full md:translate-x-0 ml-0 p-6 md:p-0 md:-ml-2 md:h-[calc(100vh-3.5rem)] w-5/6 md:w-full shrink-0 overflow-y-auto border-r border-border md:sticky"
  :aria-hidden="!showSidebar" :class="{ 'translate-x-0': showSidebar }">

    <a href="../../index.html" class="!justify-start text-sm md:!hidden bg-background"><span class="font-bold text-clip whitespace-nowrap">pythiatransformer 0.1.0 documentation</span>
    </a>

    <div class="relative overflow-hidden md:overflow-auto my-4 md:my-0 h-[calc(100vh-8rem)] md:h-auto">
      <div class="overflow-y-auto h-full w-full relative pr-6"><nav class="table w-full min-w-full my-6 lg:my-8">
  <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../_api/modules.html">pythiatransformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_api/tests_index.html">tests</a></li>
</ul>
</li>
</ul>

</nav>
      </div>
    </div>
    <button type="button" @click="showSidebar = false"
      class="absolute md:hidden right-4 top-4 rounded-sm opacity-70 transition-opacity hover:opacity-100">
      <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" fill="currentColor"
        stroke="none" class="h-4 w-4">
        <path
          d="M480 632 284 828q-11 11-28 11t-28-11q-11-11-11-28t11-28l196-196-196-196q-11-11-11-28t11-28q11-11 28-11t28 11l196 196 196-196q11-11 28-11t28 11q11 11 11 28t-11 28L536 576l196 196q11 11 11 28t-11 28q-11 11-28 11t-28-11L480 632Z" />
      </svg>
    </button>
  </aside>
        <main class="relative py-6 lg:gap-10 lg:py-8 xl:grid xl:grid-cols-[1fr_300px]">
<div class="w-full min-w-0 mx-auto">
<nav aria-label="breadcrumbs"
     class="flex items-center mb-4 space-x-1 text-sm text-muted-foreground">
  <a class="overflow-hidden text-ellipsis whitespace-nowrap hover:text-foreground"
     href="../../index.html">
    <span class="hidden md:inline">pythiatransformer 0.1.0 documentation</span>
    <svg xmlns="http://www.w3.org/2000/svg"
         height="18"
         width="18"
         viewBox="0 96 960 960"
         aria-label="Home"
         fill="currentColor"
         stroke="none"
         class="md:hidden">
      <path d="M240 856h120V616h240v240h120V496L480 316 240 496v360Zm-80 80V456l320-240 320 240v480H520V696h-80v240H160Zm320-350Z" />
    </svg>
  </a>
  
<div class="mr-1">/</div><a class="hover:text-foreground overflow-hidden text-ellipsis whitespace-nowrap"
       href="../index.html">Module code</a>
    
<div class="mr-1">/</div><span aria-current="page"
        class="font-medium text-foreground overflow-hidden text-ellipsis whitespace-nowrap">pythiatransformer.transformer</span>
</nav>

    <div id="content" role="main">
      <h1>Source code for pythiatransformer.transformer</h1><div class="highlight"><pre>
<span></span><code><span id="line-1"><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-2"><span class="sd">Custom Transformer model for single-feature particle sequences.</span>
</span><span id="line-3">
</span><span id="line-4"><span class="sd">This model predicts the sequence of final stable particles given the</span>
</span><span id="line-5"><span class="sd">sequence of input particles with status 23. The implementation is</span>
</span><span id="line-6"><span class="sd">currently validated and designed for **one scalar feature per token**,</span>
</span><span id="line-7"><span class="sd">namely the transverse momentum `pT`.</span>
</span><span id="line-8">
</span><span id="line-9"><span class="sd">Architecture</span>
</span><span id="line-10"><span class="sd">------------</span>
</span><span id="line-11"><span class="sd">- Encoder: self-attention over the status-23 sequence to build a</span>
</span><span id="line-12"><span class="sd">  contextual &quot;memory&quot; of the event.</span>
</span><span id="line-13"><span class="sd">- Decoder: masked self-attention (causal) over the partially generated</span>
</span><span id="line-14"><span class="sd">  target sequence, plus cross-attention over the encoder memory at</span>
</span><span id="line-15"><span class="sd">  every layer.</span>
</span><span id="line-16"><span class="sd">- Heads: a regression head for `pT` and an EOS head that predicts a</span>
</span><span id="line-17"><span class="sd">  stop logit per time step.</span>
</span><span id="line-18">
</span><span id="line-19"><span class="sd">Training &amp; Inference</span>
</span><span id="line-20"><span class="sd">--------------------</span>
</span><span id="line-21"><span class="sd">- Training uses teacher forcing: the decoder input is</span>
</span><span id="line-22"><span class="sd">  ``[SOS, target[:-1]]``.</span>
</span><span id="line-23"><span class="sd">  ``Loss = MSE+BCEWithLogits``. ``Mse`` on `pT` (ignoring SOS and</span>
</span><span id="line-24"><span class="sd">  padding), ``BCEWithLogits`` on EOS (one logit per step, positioned</span>
</span><span id="line-25"><span class="sd">  immediately after the last real token).</span>
</span><span id="line-26"><span class="sd">- Inference is autoregressive: start from SOS, predict one scalar `pT`</span>
</span><span id="line-27"><span class="sd">  and an EOS probability at a time, and feed each prediction back into</span>
</span><span id="line-28"><span class="sd">  the decoder until EOS triggers or the max length is reached.</span>
</span><span id="line-29">
</span><span id="line-30"><span class="sd">Shapes</span>
</span><span id="line-31"><span class="sd">------</span>
</span><span id="line-32"><span class="sd">- Encoder input  : ``[batch, L_src, 1]``  → projected to D</span>
</span><span id="line-33"><span class="sd">- Decoder input  : ``[batch, L_tgt, 1]``  → prepend SOS →</span>
</span><span id="line-34"><span class="sd">  ``[batch, L_tgt+1, D]``</span>
</span><span id="line-35"><span class="sd">- Decoder output : ``[batch, L_tgt+1, D]``</span>
</span><span id="line-36"><span class="sd">- ``pred``       : ``[batch, L_tgt+1]`` (regressed `pT`, after</span>
</span><span id="line-37"><span class="sd">  squeeze)</span>
</span><span id="line-38"><span class="sd">- ``eos_logits`` : ``[batch, L_tgt+1]`` (one logit per step, after</span>
</span><span id="line-39"><span class="sd">  squeeze)</span>
</span><span id="line-40">
</span><span id="line-41"><span class="sd">Notes</span>
</span><span id="line-42"><span class="sd">-----</span>
</span><span id="line-43"><span class="sd">- **Single-feature constraint**: this implementation assumes exactly</span>
</span><span id="line-44"><span class="sd">  one scalar feature per token (`pT`). Passing more than one feature is</span>
</span><span id="line-45"><span class="sd">  not supported.</span>
</span><span id="line-46"><span class="sd">- Padding masks are boolean (``True=pad``) and are applied to both</span>
</span><span id="line-47"><span class="sd">  encoder and decoder; the decoder also uses a triangular causal mask.</span>
</span><span id="line-48">
</span><span id="line-49"><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-50">
</span><span id="line-51"><span class="kn">import</span><span class="w"> </span><span class="nn">gc</span>
</span><span id="line-52">
</span><span id="line-53"><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="line-54"><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="line-55"><span class="kn">from</span><span class="w"> </span><span class="nn">loguru</span><span class="w"> </span><span class="kn">import</span> <span class="n">logger</span>
</span><span id="line-56"><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">ks_2samp</span><span class="p">,</span> <span class="n">wasserstein_distance</span>
</span><span id="line-57"><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</span><span id="line-58">
</span><span id="line-59">
</span><span id="line-60"><span class="k">def</span><span class="w"> </span><span class="nf">_log_peak_memory</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span><span id="line-61"><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-62"><span class="sd">    Log the peak GPU memory allocated.</span>
</span><span id="line-63">
</span><span id="line-64"><span class="sd">    Print the maximum GPU memory allocated during the current epoch and</span>
</span><span id="line-65"><span class="sd">    reset the counter.</span>
</span><span id="line-66"><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="line-67">    <span class="n">peak_mb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">2</span>
</span><span id="line-68">    <span class="n">prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;[Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">] &quot;</span> <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
</span><span id="line-69">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2"> Max memory allocated: </span><span class="si">{</span><span class="n">peak_mb</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
</span><span id="line-70">    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>
</span><span id="line-71">
</span><span id="line-72">
</span><span id="line-73"><span class="k">def</span><span class="w"> </span><span class="nf">_log_gpu_memory</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span><span id="line-74"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Print a clear summary of GPU memory usage.&quot;&quot;&quot;</span>
</span><span id="line-75">    <span class="n">alloc_mb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">2</span>
</span><span id="line-76">    <span class="n">reserved_mb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">2</span>
</span><span id="line-77">    <span class="n">stats</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_stats</span><span class="p">()</span>
</span><span id="line-78">    <span class="n">total_alloc_mb</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="s2">&quot;allocation.all.allocated&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">2</span>
</span><span id="line-79">
</span><span id="line-80">    <span class="n">prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;[Epoca </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">] &quot;</span> <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
</span><span id="line-81">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">Memory allocated: </span><span class="si">{</span><span class="n">alloc_mb</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
</span><span id="line-82">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">Memory reserved: </span><span class="si">{</span><span class="n">reserved_mb</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
</span><span id="line-83">    <span class="nb">print</span><span class="p">(</span>
</span><span id="line-84">        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">Total memory allocated througth epochs:&quot;</span>
</span><span id="line-85">        <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">total_alloc_mb</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span>
</span><span id="line-86">    <span class="p">)</span>
</span><span id="line-87">
</span><span id="line-88">
</span><span id="line-89"><span class="k">def</span><span class="w"> </span><span class="nf">_check_type</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
</span><span id="line-90"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Check whether a variable is of the expected type.&quot;&quot;&quot;</span>
</span><span id="line-91">    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
</span><span id="line-92">        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
</span><span id="line-93">            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must be of type </span><span class="si">{</span><span class="n">t</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">var</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="line-94">        <span class="p">)</span>
</span><span id="line-95">
</span><span id="line-96">
<div class="viewcode-block" id="ParticleTransformer">
<a class="viewcode-back" href="../../_api/pythiatransformer.transformer.html#pythiatransformer.transformer.ParticleTransformer">[docs]</a>
</span><span id="line-97"><span class="k">class</span><span class="w"> </span><span class="nc">ParticleTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="line-98"><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-99"><span class="sd">    Custom PyTorch Transformer model.</span>
</span><span id="line-100">
</span><span id="line-101"><span class="sd">    Transformer taking in input particles having status 23 (i.e.</span>
</span><span id="line-102"><span class="sd">    outgoing particles of the hardest subprocess) and as target the</span>
</span><span id="line-103"><span class="sd">    final particles of the event.</span>
</span><span id="line-104"><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="line-105">
<div class="viewcode-block" id="ParticleTransformer.__init__">
<a class="viewcode-back" href="../../_api/pythiatransformer.transformer.html#pythiatransformer.transformer.ParticleTransformer.__init__">[docs]</a>
</span><span id="line-106">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="line-107">        <span class="bp">self</span><span class="p">,</span>
</span><span id="line-108">        <span class="n">train_data</span><span class="p">,</span>
</span><span id="line-109">        <span class="n">val_data</span><span class="p">,</span>
</span><span id="line-110">        <span class="n">test_data</span><span class="p">,</span>
</span><span id="line-111">        <span class="n">train_data_pad_mask</span><span class="p">,</span>
</span><span id="line-112">        <span class="n">val_data_pad_mask</span><span class="p">,</span>
</span><span id="line-113">        <span class="n">test_data_pad_mask</span><span class="p">,</span>
</span><span id="line-114">        <span class="n">num_heads</span><span class="p">,</span>
</span><span id="line-115">        <span class="n">num_encoder_layers</span><span class="p">,</span>
</span><span id="line-116">        <span class="n">num_decoder_layers</span><span class="p">,</span>
</span><span id="line-117">        <span class="n">num_units</span><span class="p">,</span>
</span><span id="line-118">        <span class="n">dropout</span><span class="p">,</span>
</span><span id="line-119">        <span class="n">activation</span><span class="p">,</span>
</span><span id="line-120">    <span class="p">):</span>
</span><span id="line-121"><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-122"><span class="sd">        Class constructor.</span>
</span><span id="line-123">
</span><span id="line-124"><span class="sd">        Parameters</span>
</span><span id="line-125"><span class="sd">        ----------</span>
</span><span id="line-126"><span class="sd">        train_data : DataLoader</span>
</span><span id="line-127"><span class="sd">            DataLoader containing the training input-target pairs.</span>
</span><span id="line-128"><span class="sd">        val_data : DataLoader</span>
</span><span id="line-129"><span class="sd">            DataLoader containing the validation input-target pairs.</span>
</span><span id="line-130"><span class="sd">        test_data : DataLoader</span>
</span><span id="line-131"><span class="sd">            DataLoader containing the test input-target pairs.</span>
</span><span id="line-132"><span class="sd">        train_data_pad_mask : DataLoader</span>
</span><span id="line-133"><span class="sd">            DataLoader containing the padding masks for the training</span>
</span><span id="line-134"><span class="sd">            set, used to mask padded values during attention</span>
</span><span id="line-135"><span class="sd">            computations.</span>
</span><span id="line-136"><span class="sd">        val_data_pad_mask : DataLoader</span>
</span><span id="line-137"><span class="sd">            DataLoader containing the padding masks for the validation</span>
</span><span id="line-138"><span class="sd">            set.</span>
</span><span id="line-139"><span class="sd">        test_data_pad_mask : DataLoader</span>
</span><span id="line-140"><span class="sd">            DataLoader containing the padding masks for the test set.</span>
</span><span id="line-141"><span class="sd">        num_heads : int</span>
</span><span id="line-142"><span class="sd">            Number of attention heads in the transformer architecture.</span>
</span><span id="line-143"><span class="sd">        num_encoder_layers : int</span>
</span><span id="line-144"><span class="sd">            Number of encoder layers.</span>
</span><span id="line-145"><span class="sd">        num_decoder_layers : int</span>
</span><span id="line-146"><span class="sd">            Number of decoder layers.</span>
</span><span id="line-147"><span class="sd">        num_units : int</span>
</span><span id="line-148"><span class="sd">            Number of units in each hidden layer. Typically chosen</span>
</span><span id="line-149"><span class="sd">            greater than the number of input features to enable more</span>
</span><span id="line-150"><span class="sd">            abstract representations.</span>
</span><span id="line-151"><span class="sd">        dropout : float</span>
</span><span id="line-152"><span class="sd">            Dropout probability for each neuron.</span>
</span><span id="line-153"><span class="sd">        activation : nn.Module</span>
</span><span id="line-154"><span class="sd">            Activation function used in encoder and/or decoder layers.</span>
</span><span id="line-155"><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="line-156">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="line-157">        <span class="bp">self</span><span class="o">.</span><span class="n">train_data</span> <span class="o">=</span> <span class="n">train_data</span>
</span><span id="line-158">        <span class="bp">self</span><span class="o">.</span><span class="n">val_data</span> <span class="o">=</span> <span class="n">val_data</span>
</span><span id="line-159">        <span class="bp">self</span><span class="o">.</span><span class="n">test_data</span> <span class="o">=</span> <span class="n">test_data</span>
</span><span id="line-160">        <span class="bp">self</span><span class="o">.</span><span class="n">train_data_pad_mask</span> <span class="o">=</span> <span class="n">train_data_pad_mask</span>
</span><span id="line-161">        <span class="bp">self</span><span class="o">.</span><span class="n">val_data_pad_mask</span> <span class="o">=</span> <span class="n">val_data_pad_mask</span>
</span><span id="line-162">        <span class="bp">self</span><span class="o">.</span><span class="n">test_data_pad_mask</span> <span class="o">=</span> <span class="n">test_data_pad_mask</span>
</span><span id="line-163">        <span class="bp">self</span><span class="o">.</span><span class="n">num_encoder_layers</span> <span class="o">=</span> <span class="n">num_encoder_layers</span>
</span><span id="line-164">
</span><span id="line-165">        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span><span id="line-166">        <span class="bp">self</span><span class="o">.</span><span class="n">num_decoder_layers</span> <span class="o">=</span> <span class="n">num_decoder_layers</span>
</span><span id="line-167">        <span class="bp">self</span><span class="o">.</span><span class="n">num_units</span> <span class="o">=</span> <span class="n">num_units</span>
</span><span id="line-168">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
</span><span id="line-169">        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
</span><span id="line-170">
</span><span id="line-171">        <span class="c1"># Type controls</span>
</span><span id="line-172">        <span class="n">_check_type</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="s2">&quot;num_heads&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
</span><span id="line-173">        <span class="n">_check_type</span><span class="p">(</span><span class="n">num_encoder_layers</span><span class="p">,</span> <span class="s2">&quot;num_encoder_layers&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
</span><span id="line-174">        <span class="n">_check_type</span><span class="p">(</span><span class="n">num_decoder_layers</span><span class="p">,</span> <span class="s2">&quot;num_decoder_layers&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
</span><span id="line-175">        <span class="n">_check_type</span><span class="p">(</span><span class="n">num_units</span><span class="p">,</span> <span class="s2">&quot;num_units&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
</span><span id="line-176">        <span class="n">_check_type</span><span class="p">(</span><span class="n">dropout</span><span class="p">,</span> <span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span>
</span><span id="line-177">
</span><span id="line-178">        <span class="c1"># Data tensor shape and type control.</span>
</span><span id="line-179">        <span class="n">DIM_TENSOR</span> <span class="o">=</span> <span class="mi">3</span>
</span><span id="line-180">        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">loader</span> <span class="ow">in</span> <span class="p">[</span>
</span><span id="line-181">            <span class="p">(</span><span class="s2">&quot;train_data&quot;</span><span class="p">,</span> <span class="n">train_data</span><span class="p">),</span>
</span><span id="line-182">            <span class="p">(</span><span class="s2">&quot;val_data&quot;</span><span class="p">,</span> <span class="n">val_data</span><span class="p">),</span>
</span><span id="line-183">            <span class="p">(</span><span class="s2">&quot;test_data&quot;</span><span class="p">,</span> <span class="n">test_data</span><span class="p">),</span>
</span><span id="line-184">        <span class="p">]:</span>
</span><span id="line-185">            <span class="n">_check_type</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">)</span>
</span><span id="line-186">            <span class="c1"># Verify a single batch.</span>
</span><span id="line-187">            <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">loader</span><span class="p">))</span>
</span><span id="line-188">            <span class="n">x</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="line-189">            <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">DIM_TENSOR</span><span class="p">,</span> <span class="p">(</span>
</span><span id="line-190">                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must have </span><span class="si">{</span><span class="n">DIM_TENSOR</span><span class="si">}</span><span class="s2"> dimensions [B, L, F],&quot;</span>
</span><span id="line-191">                <span class="sa">f</span><span class="s2">&quot; got </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="line-192">            <span class="p">)</span>
</span><span id="line-193">            <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span>
</span><span id="line-194">                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must have exactly 1 feature, got </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="line-195">            <span class="p">)</span>
</span><span id="line-196">
</span><span id="line-197">        <span class="k">if</span> <span class="ow">not</span> <span class="n">num_units</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="line-198">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="line-199">                <span class="s2">&quot;Number of units must be a multiple of number of heads.&quot;</span>
</span><span id="line-200">            <span class="p">)</span>
</span><span id="line-201">        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">dropout</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">:</span>
</span><span id="line-202">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Dropout must be between 0.0 and 1.0&quot;</span><span class="p">)</span>
</span><span id="line-203">
</span><span id="line-204">        <span class="c1"># Loss functions used during the training</span>
</span><span id="line-205">        <span class="bp">self</span><span class="o">.</span><span class="n">mse_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>
</span><span id="line-206">        <span class="bp">self</span><span class="o">.</span><span class="n">bce_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
</span><span id="line-207">
</span><span id="line-208">        <span class="bp">self</span><span class="o">.</span><span class="n">build_projection_layer</span><span class="p">()</span>
</span><span id="line-209">        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_transformer</span><span class="p">()</span>
</span><span id="line-210">
</span><span id="line-211">        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>
</span><span id="line-212">        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model initialized on device: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>

</span><span id="line-213">
<div class="viewcode-block" id="ParticleTransformer.build_projection_layer">
<a class="viewcode-back" href="../../_api/pythiatransformer.transformer.html#pythiatransformer.transformer.ParticleTransformer.build_projection_layer">[docs]</a>
</span><span id="line-214">    <span class="k">def</span><span class="w"> </span><span class="nf">build_projection_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="line-215"><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-216"><span class="sd">        Build the input/output projection layers.</span>
</span><span id="line-217">
</span><span id="line-218"><span class="sd">        Initializes the layers responible for projecting input features</span>
</span><span id="line-219"><span class="sd">        to the model&#39;s hidden dimentionality and outputs back to the</span>
</span><span id="line-220"><span class="sd">        original feature space.</span>
</span><span id="line-221"><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="line-222">        <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="p">)</span>
</span><span id="line-223">        <span class="bp">self</span><span class="o">.</span><span class="n">sos_token</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="p">))</span>
</span><span id="line-224">        <span class="bp">self</span><span class="o">.</span><span class="n">particle_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="line-225">        <span class="c1"># The EOS head returns one value per feature.</span>
</span><span id="line-226">        <span class="bp">self</span><span class="o">.</span><span class="n">eos_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="line-227">        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Projection layers input/output created.&quot;</span><span class="p">)</span></div>

</span><span id="line-228">
<div class="viewcode-block" id="ParticleTransformer.initialize_transformer">
<a class="viewcode-back" href="../../_api/pythiatransformer.transformer.html#pythiatransformer.transformer.ParticleTransformer.initialize_transformer">[docs]</a>
</span><span id="line-229">    <span class="k">def</span><span class="w"> </span><span class="nf">initialize_transformer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="line-230"><span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the transformer with the specified parameters.&quot;&quot;&quot;</span>
</span><span id="line-231">        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Transformer</span><span class="p">(</span>
</span><span id="line-232">            <span class="n">d_model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="p">,</span>
</span><span id="line-233">            <span class="n">nhead</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
</span><span id="line-234">            <span class="n">num_encoder_layers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_encoder_layers</span><span class="p">,</span>
</span><span id="line-235">            <span class="n">num_decoder_layers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_decoder_layers</span><span class="p">,</span>
</span><span id="line-236">            <span class="n">dim_feedforward</span><span class="o">=</span><span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="p">,</span>
</span><span id="line-237">            <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
</span><span id="line-238">            <span class="n">activation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">,</span>
</span><span id="line-239">            <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="line-240">        <span class="p">)</span>
</span><span id="line-241">
</span><span id="line-242">        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
</span><span id="line-243">            <span class="sa">f</span><span class="s2">&quot;Transformer initialized with &quot;</span>
</span><span id="line-244">            <span class="sa">f</span><span class="s2">&quot;number of hidden units: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="si">}</span><span class="s2">, &quot;</span>
</span><span id="line-245">            <span class="sa">f</span><span class="s2">&quot;number of heads: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">, &quot;</span>
</span><span id="line-246">            <span class="sa">f</span><span class="s2">&quot;number of encoder layers: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_encoder_layers</span><span class="si">}</span><span class="s2">, &quot;</span>
</span><span id="line-247">            <span class="sa">f</span><span class="s2">&quot;number of decoder layers: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_decoder_layers</span><span class="si">}</span><span class="s2">, &quot;</span>
</span><span id="line-248">            <span class="sa">f</span><span class="s2">&quot;number of feedforward units: </span><span class="si">{</span><span class="mi">4</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="si">}</span><span class="s2">, &quot;</span>
</span><span id="line-249">            <span class="sa">f</span><span class="s2">&quot;dropout probability: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="si">}</span><span class="s2">, &quot;</span>
</span><span id="line-250">            <span class="sa">f</span><span class="s2">&quot;activation function: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="line-251">        <span class="p">)</span></div>

</span><span id="line-252">
<div class="viewcode-block" id="ParticleTransformer.forward">
<a class="viewcode-back" href="../../_api/pythiatransformer.transformer.html#pythiatransformer.transformer.ParticleTransformer.forward">[docs]</a>
</span><span id="line-253">    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">enc_input_mask</span><span class="p">,</span> <span class="n">dec_input_mask</span><span class="p">):</span>
</span><span id="line-254"><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-255"><span class="sd">        Build a custom forward method.</span>
</span><span id="line-256">
</span><span id="line-257"><span class="sd">        Computes the output of the model by projecting both input</span>
</span><span id="line-258"><span class="sd">        and target into an hidden representation space, processing them</span>
</span><span id="line-259"><span class="sd">        through a Transformer, and projecting the result back to the</span>
</span><span id="line-260"><span class="sd">        original feature space.</span>
</span><span id="line-261">
</span><span id="line-262"><span class="sd">        Parameters</span>
</span><span id="line-263"><span class="sd">        ----------</span>
</span><span id="line-264"><span class="sd">        input : torch.Tensor</span>
</span><span id="line-265"><span class="sd">            Input tensor representing status 23 particles used as the</span>
</span><span id="line-266"><span class="sd">            encoder input sequence.</span>
</span><span id="line-267"><span class="sd">        target : torch.Tensor</span>
</span><span id="line-268"><span class="sd">            Tensor of stable final-state particles used as the decoder</span>
</span><span id="line-269"><span class="sd">            target.</span>
</span><span id="line-270"><span class="sd">        enc_input_mask : torch.Tensor</span>
</span><span id="line-271"><span class="sd">            Boolean mask applied to encoder input to ignore padding</span>
</span><span id="line-272"><span class="sd">            tokens.</span>
</span><span id="line-273"><span class="sd">        dec_input_mask : torch.Tensor</span>
</span><span id="line-274"><span class="sd">            Boolean mask applied to decoder input to ignore padding</span>
</span><span id="line-275"><span class="sd">            tokens.</span>
</span><span id="line-276">
</span><span id="line-277"><span class="sd">        Returns</span>
</span><span id="line-278"><span class="sd">        -------</span>
</span><span id="line-279"><span class="sd">        pred : torch.Tensor</span>
</span><span id="line-280"><span class="sd">            The output tensor after processing through the model.</span>
</span><span id="line-281"><span class="sd">        eos_prob_vector : torch.Tensor</span>
</span><span id="line-282"><span class="sd">            End-of-sequence probabilities for each step.</span>
</span><span id="line-283"><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="line-284">        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="line-285">
</span><span id="line-286">        <span class="c1"># Input projected.</span>
</span><span id="line-287">        <span class="n">enc_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="line-288">
</span><span id="line-289">        <span class="c1"># SOS projected.</span>
</span><span id="line-290">        <span class="n">sos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sos_token</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="line-291">
</span><span id="line-292">        <span class="c1"># Decoder input projected without SOS. Then concatenate SOS to</span>
</span><span id="line-293">        <span class="c1"># dec_input.</span>
</span><span id="line-294">        <span class="n">dec_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
</span><span id="line-295">        <span class="n">dec_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">sos</span><span class="p">,</span> <span class="n">dec_input</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="line-296">
</span><span id="line-297">        <span class="c1"># Encoder and decoder input padding mask, decoder triangular</span>
</span><span id="line-298">        <span class="c1"># causal mask for self-attention.</span>
</span><span id="line-299">        <span class="n">sos_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-300">        <span class="n">dec_input_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">sos_mask</span><span class="p">,</span> <span class="n">dec_input_mask</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="line-301">        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Transformer</span><span class="o">.</span><span class="n">generate_square_subsequent_mask</span><span class="p">(</span>
</span><span id="line-302">            <span class="n">dec_input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="line-303">        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-304">
</span><span id="line-305">        <span class="c1"># Compute the output and project it to the original feature</span>
</span><span id="line-306">        <span class="c1"># space.</span>
</span><span id="line-307">        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span>
</span><span id="line-308">            <span class="n">src</span><span class="o">=</span><span class="n">enc_input</span><span class="p">,</span>
</span><span id="line-309">            <span class="n">tgt</span><span class="o">=</span><span class="n">dec_input</span><span class="p">,</span>
</span><span id="line-310">            <span class="n">tgt_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
</span><span id="line-311">            <span class="n">src_key_padding_mask</span><span class="o">=</span><span class="n">enc_input_mask</span><span class="p">,</span>
</span><span id="line-312">            <span class="n">tgt_key_padding_mask</span><span class="o">=</span><span class="n">dec_input_mask</span><span class="p">,</span>
</span><span id="line-313">        <span class="p">)</span>
</span><span id="line-314">        <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">particle_head</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="line-315">        <span class="n">eos_prob_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_head</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="line-316">        <span class="k">return</span> <span class="n">pred</span><span class="p">,</span> <span class="n">eos_prob_vector</span></div>

</span><span id="line-317">
<div class="viewcode-block" id="ParticleTransformer.train_one_epoch">
<a class="viewcode-back" href="../../_api/pythiatransformer.transformer.html#pythiatransformer.transformer.ParticleTransformer.train_one_epoch">[docs]</a>
</span><span id="line-318">    <span class="k">def</span><span class="w"> </span><span class="nf">train_one_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">optim</span><span class="p">):</span>
</span><span id="line-319"><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-320"><span class="sd">        Train the model for one epoch.</span>
</span><span id="line-321">
</span><span id="line-322"><span class="sd">        This method runs one full epoch of training over the dataset.</span>
</span><span id="line-323"><span class="sd">        It performs the following steps for each batch:</span>
</span><span id="line-324">
</span><span id="line-325"><span class="sd">        1. Move input data and masks to the correct device.</span>
</span><span id="line-326"><span class="sd">        2. Compute the model output and EOS (end-of-sequence)</span>
</span><span id="line-327"><span class="sd">           probabilities.</span>
</span><span id="line-328"><span class="sd">        3. Compute the regression loss (MSE) over valid particles</span>
</span><span id="line-329"><span class="sd">           only.</span>
</span><span id="line-330"><span class="sd">        4. Compute the EOS classification loss (BCE) over all</span>
</span><span id="line-331"><span class="sd">           tokens.</span>
</span><span id="line-332"><span class="sd">        5. Backpropagate and update model parameters.</span>
</span><span id="line-333">
</span><span id="line-334"><span class="sd">        Notes</span>
</span><span id="line-335"><span class="sd">        -----</span>
</span><span id="line-336"><span class="sd">        - **Masked MSE loss**: computed only on real (non-padding)</span>
</span><span id="line-337"><span class="sd">          particles. The loss is averaged over the number of *valid</span>
</span><span id="line-338"><span class="sd">          tokens*, not the batch size. This prevents the loss from</span>
</span><span id="line-339"><span class="sd">          being diluted when padding is present.</span>
</span><span id="line-340"><span class="sd">        - **BCE loss**: computed on every sequence position because</span>
</span><span id="line-341"><span class="sd">          each token (including padding) has a meaningful EOS label</span>
</span><span id="line-342"><span class="sd">          (0 = not stop, 1 = stop).</span>
</span><span id="line-343"><span class="sd">        - Both losses are thus *token-level averages* and are directly</span>
</span><span id="line-344"><span class="sd">          comparable without additional normalization by batch size.</span>
</span><span id="line-345">
</span><span id="line-346"><span class="sd">        Parameters</span>
</span><span id="line-347"><span class="sd">        ----------</span>
</span><span id="line-348"><span class="sd">        epoch : int</span>
</span><span id="line-349"><span class="sd">            Current epoch number.</span>
</span><span id="line-350"><span class="sd">        optim : torch.optim.Optimizer</span>
</span><span id="line-351"><span class="sd">            Optimizer used for parameter updates.</span>
</span><span id="line-352">
</span><span id="line-353"><span class="sd">        Returns</span>
</span><span id="line-354"><span class="sd">        -------</span>
</span><span id="line-355"><span class="sd">        loss_epoch : float</span>
</span><span id="line-356"><span class="sd">            Mean training loss over all batches for this epoch.</span>
</span><span id="line-357"><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="line-358">        <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span><span id="line-359">        <span class="n">loss_epoch</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="line-360">
</span><span id="line-361">        <span class="k">for</span> <span class="p">(</span><span class="n">enc_input_batch</span><span class="p">,</span> <span class="n">dec_input_batch</span><span class="p">),</span> <span class="p">(</span>
</span><span id="line-362">            <span class="n">enc_input_padding_mask_batch</span><span class="p">,</span>
</span><span id="line-363">            <span class="n">dec_input_padding_mask_batch</span><span class="p">,</span>
</span><span id="line-364">        <span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_data_pad_mask</span><span class="p">):</span>
</span><span id="line-365">            <span class="c1"># Move tensors to GPU/CPU as needed.</span>
</span><span id="line-366">            <span class="n">enc_input</span> <span class="o">=</span> <span class="n">enc_input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-367">            <span class="n">dec_input</span> <span class="o">=</span> <span class="n">dec_input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-368">            <span class="n">enc_input_padding_mask</span> <span class="o">=</span> <span class="n">enc_input_padding_mask_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="line-369">                <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="line-370">            <span class="p">)</span>
</span><span id="line-371">            <span class="n">dec_input_padding_mask</span> <span class="o">=</span> <span class="n">dec_input_padding_mask_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="line-372">                <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="line-373">            <span class="p">)</span>
</span><span id="line-374">
</span><span id="line-375">            <span class="c1"># Invert the padding mask: True where tokens are valid.</span>
</span><span id="line-376">            <span class="n">inverse_dec_input_padding_mask</span> <span class="o">=</span> <span class="o">~</span><span class="n">dec_input_padding_mask</span>
</span><span id="line-377">
</span><span id="line-378">            <span class="c1"># Build EOS targets: one &quot;1&quot; at the true end-of-sequence</span>
</span><span id="line-379">            <span class="c1"># position for each event.</span>
</span><span id="line-380">            <span class="n">eos_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="line-381">                <span class="n">dec_input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">dec_input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dec_input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span><span id="line-382">            <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-383">            <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dec_input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
</span><span id="line-384">                <span class="n">len_event</span> <span class="o">=</span> <span class="n">inverse_dec_input_padding_mask</span><span class="p">[</span><span class="n">event</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span><span id="line-385">                <span class="n">eos_tensor</span><span class="p">[</span><span class="n">event</span><span class="p">,</span> <span class="n">len_event</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="line-386">
</span><span id="line-387">            <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span><span id="line-388">
</span><span id="line-389">            <span class="c1"># Forward pass.</span>
</span><span id="line-390">            <span class="n">output</span><span class="p">,</span> <span class="n">eos_prob_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="line-391">                <span class="n">enc_input</span><span class="p">,</span>
</span><span id="line-392">                <span class="n">dec_input</span><span class="p">,</span>
</span><span id="line-393">                <span class="n">enc_input_padding_mask</span><span class="p">,</span>
</span><span id="line-394">                <span class="n">dec_input_padding_mask</span><span class="p">,</span>
</span><span id="line-395">            <span class="p">)</span>  <span class="c1"># shape: (batch_size, max_particles).</span>
</span><span id="line-396">
</span><span id="line-397">            <span class="c1"># 1) Masked MSE regression loss.</span>
</span><span id="line-398">            <span class="c1"># Compute elementwise squared error without internal</span>
</span><span id="line-399">            <span class="c1"># averaging.</span>
</span><span id="line-400">            <span class="n">mse_elements</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span>
</span><span id="line-401">                <span class="n">output</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span>  <span class="c1"># skip SOS token at position 0.</span>
</span><span id="line-402">                <span class="n">dec_input</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
</span><span id="line-403">            <span class="p">)</span>  <span class="c1"># shape: (batch_size, max_particles).</span>
</span><span id="line-404">
</span><span id="line-405">            <span class="c1"># Build mask for valid tokens.</span>
</span><span id="line-406">            <span class="c1"># Shape: (batch_size, max_particles).</span>
</span><span id="line-407">            <span class="n">mask</span> <span class="o">=</span> <span class="n">inverse_dec_input_padding_mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span id="line-408">            <span class="n">valid</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span><span id="line-409">
</span><span id="line-410">            <span class="c1"># Average only over real (non-padding) tokens.</span>
</span><span id="line-411">            <span class="n">mse</span> <span class="o">=</span> <span class="p">(</span><span class="n">mse_elements</span> <span class="o">*</span> <span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">valid</span>
</span><span id="line-412">
</span><span id="line-413">            <span class="c1"># 2) BCE loss for EOS prediction.</span>
</span><span id="line-414">            <span class="c1"># Each time step has a valid label (EOS=0 or 1), so no</span>
</span><span id="line-415">            <span class="c1"># masking is needed.</span>
</span><span id="line-416">            <span class="n">bce</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bce_loss</span><span class="p">(</span><span class="n">eos_prob_vector</span><span class="p">,</span> <span class="n">eos_tensor</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span><span id="line-417">
</span><span id="line-418">            <span class="c1"># 3) Combined loss.</span>
</span><span id="line-419">            <span class="c1"># Both terms are averaged per token so directly comparable</span>
</span><span id="line-420">            <span class="c1"># in scale.</span>
</span><span id="line-421">            <span class="n">loss</span> <span class="o">=</span> <span class="n">mse</span> <span class="o">+</span> <span class="n">bce</span>
</span><span id="line-422">
</span><span id="line-423">            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
</span><span id="line-424">                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loss is not finite at epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="line-425">
</span><span id="line-426">            <span class="c1"># Backpropagation and optimization.</span>
</span><span id="line-427">            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span><span id="line-428">            <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span><span id="line-429">            <span class="n">loss_epoch</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span><span id="line-430">
</span><span id="line-431">            <span class="c1"># Free memory and clear cache between batches.</span>
</span><span id="line-432">            <span class="k">del</span> <span class="p">(</span>
</span><span id="line-433">                <span class="n">enc_input</span><span class="p">,</span>
</span><span id="line-434">                <span class="n">dec_input</span><span class="p">,</span>
</span><span id="line-435">                <span class="n">enc_input_padding_mask</span><span class="p">,</span>
</span><span id="line-436">                <span class="n">dec_input_padding_mask</span><span class="p">,</span>
</span><span id="line-437">                <span class="n">output</span><span class="p">,</span>
</span><span id="line-438">                <span class="n">eos_prob_vector</span><span class="p">,</span>
</span><span id="line-439">                <span class="n">eos_tensor</span><span class="p">,</span>
</span><span id="line-440">                <span class="n">loss</span><span class="p">,</span>
</span><span id="line-441">                <span class="n">mse</span><span class="p">,</span>
</span><span id="line-442">                <span class="n">bce</span><span class="p">,</span>
</span><span id="line-443">                <span class="n">inverse_dec_input_padding_mask</span><span class="p">,</span>
</span><span id="line-444">            <span class="p">)</span>
</span><span id="line-445">            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</span><span id="line-446">
</span><span id="line-447">        <span class="c1"># Manual garbage collection.</span>
</span><span id="line-448">        <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
</span><span id="line-449">        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</span><span id="line-450">
</span><span id="line-451">        <span class="c1"># Average loss over the number of batches.</span>
</span><span id="line-452">        <span class="n">loss_epoch</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_data</span><span class="p">)</span>
</span><span id="line-453">        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training loss at epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss_epoch</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="line-454">        <span class="k">return</span> <span class="n">loss_epoch</span></div>

</span><span id="line-455">
<div class="viewcode-block" id="ParticleTransformer.val_one_epoch">
<a class="viewcode-back" href="../../_api/pythiatransformer.transformer.html#pythiatransformer.transformer.ParticleTransformer.val_one_epoch">[docs]</a>
</span><span id="line-456">    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
</span><span id="line-457">    <span class="k">def</span><span class="w"> </span><span class="nf">val_one_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
</span><span id="line-458"><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-459"><span class="sd">        Validate the model for one epoch.</span>
</span><span id="line-460">
</span><span id="line-461"><span class="sd">        This method runs one full pass over the validation dataset</span>
</span><span id="line-462"><span class="sd">        without gradient computation. For each batch, it repeats the</span>
</span><span id="line-463"><span class="sd">        steps of ``ParticleTransformer.train_one_epoch`` without</span>
</span><span id="line-464"><span class="sd">        backpropagation and weights update.</span>
</span><span id="line-465">
</span><span id="line-466"><span class="sd">        Parameters</span>
</span><span id="line-467"><span class="sd">        ----------</span>
</span><span id="line-468"><span class="sd">        epoch : int</span>
</span><span id="line-469"><span class="sd">            Current epoch number, used for logging only.</span>
</span><span id="line-470">
</span><span id="line-471"><span class="sd">        Returns</span>
</span><span id="line-472"><span class="sd">        -------</span>
</span><span id="line-473"><span class="sd">        loss_epoch : float</span>
</span><span id="line-474"><span class="sd">            Mean validation loss over all batches for this epoch.</span>
</span><span id="line-475"><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="line-476">        <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span><span id="line-477">        <span class="n">loss_epoch</span> <span class="o">=</span> <span class="mf">0.0</span>
</span><span id="line-478">
</span><span id="line-479">        <span class="k">for</span> <span class="p">(</span><span class="n">enc_input_batch</span><span class="p">,</span> <span class="n">dec_input_batch</span><span class="p">),</span> <span class="p">(</span>
</span><span id="line-480">            <span class="n">enc_input_padding_mask_batch</span><span class="p">,</span>
</span><span id="line-481">            <span class="n">dec_input_padding_mask_batch</span><span class="p">,</span>
</span><span id="line-482">        <span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">val_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">val_data_pad_mask</span><span class="p">):</span>
</span><span id="line-483">            <span class="c1"># Move tensors to GPU/CPU as needed.</span>
</span><span id="line-484">            <span class="n">enc_input</span> <span class="o">=</span> <span class="n">enc_input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-485">            <span class="n">dec_input</span> <span class="o">=</span> <span class="n">dec_input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-486">            <span class="n">enc_input_padding_mask</span> <span class="o">=</span> <span class="n">enc_input_padding_mask_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="line-487">                <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="line-488">            <span class="p">)</span>
</span><span id="line-489">            <span class="n">dec_input_padding_mask</span> <span class="o">=</span> <span class="n">dec_input_padding_mask_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
</span><span id="line-490">                <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="line-491">            <span class="p">)</span>
</span><span id="line-492">
</span><span id="line-493">            <span class="c1"># Inverted mask: True on real tokens (non-padding).</span>
</span><span id="line-494">            <span class="n">inverse_dec_input_padding_mask</span> <span class="o">=</span> <span class="o">~</span><span class="n">dec_input_padding_mask</span>
</span><span id="line-495">
</span><span id="line-496">            <span class="c1"># Build EOS target.</span>
</span><span id="line-497">            <span class="n">eos_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span><span id="line-498">                <span class="n">dec_input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
</span><span id="line-499">                <span class="n">dec_input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="line-500">                <span class="n">dec_input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
</span><span id="line-501">                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
</span><span id="line-502">            <span class="p">)</span>
</span><span id="line-503">            <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dec_input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
</span><span id="line-504">                <span class="n">len_event</span> <span class="o">=</span> <span class="n">inverse_dec_input_padding_mask</span><span class="p">[</span><span class="n">event</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span><span id="line-505">                <span class="n">eos_tensor</span><span class="p">[</span><span class="n">event</span><span class="p">,</span> <span class="n">len_event</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="line-506">
</span><span id="line-507">            <span class="c1"># Forward pass (no grad due to @torch.no_grad()).</span>
</span><span id="line-508">            <span class="n">output</span><span class="p">,</span> <span class="n">eos_prob_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span><span id="line-509">                <span class="n">enc_input</span><span class="p">,</span>
</span><span id="line-510">                <span class="n">dec_input</span><span class="p">,</span>
</span><span id="line-511">                <span class="n">enc_input_padding_mask</span><span class="p">,</span>
</span><span id="line-512">                <span class="n">dec_input_padding_mask</span><span class="p">,</span>
</span><span id="line-513">            <span class="p">)</span>
</span><span id="line-514">
</span><span id="line-515">            <span class="c1"># Masked MSE regression loss.</span>
</span><span id="line-516">            <span class="n">mse_elements</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">output</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">dec_input</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span><span id="line-517">
</span><span id="line-518">            <span class="n">mask</span> <span class="o">=</span> <span class="n">inverse_dec_input_padding_mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span id="line-519">            <span class="n">valid</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span><span id="line-520">            <span class="n">mse</span> <span class="o">=</span> <span class="p">(</span><span class="n">mse_elements</span> <span class="o">*</span> <span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">valid</span>
</span><span id="line-521">
</span><span id="line-522">            <span class="c1"># BCE for EOS.</span>
</span><span id="line-523">            <span class="n">bce</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bce_loss</span><span class="p">(</span><span class="n">eos_prob_vector</span><span class="p">,</span> <span class="n">eos_tensor</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span><span id="line-524">
</span><span id="line-525">            <span class="c1"># Combined loss.</span>
</span><span id="line-526">            <span class="n">loss</span> <span class="o">=</span> <span class="n">mse</span> <span class="o">+</span> <span class="n">bce</span>
</span><span id="line-527">
</span><span id="line-528">            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
</span><span id="line-529">                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="line-530">                    <span class="sa">f</span><span class="s2">&quot;Validation loss is not finite at epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="line-531">                <span class="p">)</span>
</span><span id="line-532">
</span><span id="line-533">            <span class="n">loss_epoch</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span><span id="line-534">
</span><span id="line-535">            <span class="k">del</span> <span class="p">(</span>
</span><span id="line-536">                <span class="n">enc_input</span><span class="p">,</span>
</span><span id="line-537">                <span class="n">dec_input</span><span class="p">,</span>
</span><span id="line-538">                <span class="n">enc_input_padding_mask</span><span class="p">,</span>
</span><span id="line-539">                <span class="n">dec_input_padding_mask</span><span class="p">,</span>
</span><span id="line-540">                <span class="n">output</span><span class="p">,</span>
</span><span id="line-541">                <span class="n">eos_prob_vector</span><span class="p">,</span>
</span><span id="line-542">                <span class="n">eos_tensor</span><span class="p">,</span>
</span><span id="line-543">                <span class="n">loss</span><span class="p">,</span>
</span><span id="line-544">                <span class="n">mse</span><span class="p">,</span>
</span><span id="line-545">                <span class="n">bce</span><span class="p">,</span>
</span><span id="line-546">                <span class="n">inverse_dec_input_padding_mask</span><span class="p">,</span>
</span><span id="line-547">            <span class="p">)</span>
</span><span id="line-548">            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</span><span id="line-549">
</span><span id="line-550">        <span class="c1"># Mean over validation batches.</span>
</span><span id="line-551">        <span class="n">loss_epoch</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">val_data</span><span class="p">)</span>
</span><span id="line-552">        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
</span><span id="line-553">            <span class="sa">f</span><span class="s2">&quot;Validation loss at epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss_epoch</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="line-554">        <span class="p">)</span>
</span><span id="line-555">        <span class="k">return</span> <span class="n">loss_epoch</span></div>

</span><span id="line-556">
<div class="viewcode-block" id="ParticleTransformer.train_val">
<a class="viewcode-back" href="../../_api/pythiatransformer.transformer.html#pythiatransformer.transformer.ParticleTransformer.train_val">[docs]</a>
</span><span id="line-557">    <span class="k">def</span><span class="w"> </span><span class="nf">train_val</span><span class="p">(</span>
</span><span id="line-558">        <span class="bp">self</span><span class="p">,</span>
</span><span id="line-559">        <span class="n">num_epochs</span><span class="p">,</span>
</span><span id="line-560">        <span class="n">optim</span><span class="p">,</span>
</span><span id="line-561">        <span class="n">patient_early</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
</span><span id="line-562">    <span class="p">):</span>
</span><span id="line-563"><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-564"><span class="sd">        Implement the training and validation loop.</span>
</span><span id="line-565">
</span><span id="line-566"><span class="sd">        Trains and validates the model for the given number of epochs,</span>
</span><span id="line-567"><span class="sd">        using ``train_one_epoch`` and ``loss_one_epoch`` methods.</span>
</span><span id="line-568"><span class="sd">        Implements early stopping if validation loss does not improve.</span>
</span><span id="line-569">
</span><span id="line-570"><span class="sd">        Parameters</span>
</span><span id="line-571"><span class="sd">        ----------</span>
</span><span id="line-572"><span class="sd">        num_epochs: int</span>
</span><span id="line-573"><span class="sd">            Number of total epochs.</span>
</span><span id="line-574"><span class="sd">        optim: torch.optim.optimizer</span>
</span><span id="line-575"><span class="sd">            Optimizer used for updating model parameters.</span>
</span><span id="line-576"><span class="sd">        patient: int</span>
</span><span id="line-577"><span class="sd">            Number of consecutive epochs to wait for an improvement in</span>
</span><span id="line-578"><span class="sd">            validation loss before stopping early. Default 10.</span>
</span><span id="line-579"><span class="sd">        Returns</span>
</span><span id="line-580"><span class="sd">        -------</span>
</span><span id="line-581"><span class="sd">        train_loss: list[float]</span>
</span><span id="line-582"><span class="sd">            Training loss recorded at each epoch.</span>
</span><span id="line-583"><span class="sd">        val_loss: list[float]</span>
</span><span id="line-584"><span class="sd">            Validation loss recorded at each epoch.</span>
</span><span id="line-585"><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="line-586">        <span class="n">_check_type</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">,</span> <span class="s2">&quot;num_epochs&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
</span><span id="line-587">        <span class="n">_check_type</span><span class="p">(</span><span class="n">patient_early</span><span class="p">,</span> <span class="s2">&quot;patient_early&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
</span><span id="line-588">
</span><span id="line-589">        <span class="k">if</span> <span class="ow">not</span> <span class="n">patient_early</span> <span class="o">&lt;</span> <span class="n">num_epochs</span><span class="p">:</span>
</span><span id="line-590">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="line-591">                <span class="s2">&quot;Patient must be smaller than the number of epochs.&quot;</span>
</span><span id="line-592">            <span class="p">)</span>
</span><span id="line-593">
</span><span id="line-594">        <span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="line-595">        <span class="n">val_loss</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="line-596">
</span><span id="line-597">        <span class="n">counter_earlystop</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="line-598">
</span><span id="line-599">        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Training started!&quot;</span><span class="p">)</span>
</span><span id="line-600">        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span><span id="line-601">            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
</span><span id="line-602">                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>
</span><span id="line-603">            <span class="n">train_loss_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_one_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
</span><span id="line-604">            <span class="n">val_loss_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">val_one_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
</span><span id="line-605">            <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss_epoch</span><span class="p">)</span>
</span><span id="line-606">            <span class="n">val_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_loss_epoch</span><span class="p">)</span>
</span><span id="line-607">
</span><span id="line-608">            <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_epochs</span> <span class="o">/</span> <span class="mi">10</span><span class="p">):</span>
</span><span id="line-609">                <span class="n">stop_early</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="n">val_loss</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
</span><span id="line-610">                <span class="k">if</span> <span class="n">stop_early</span><span class="p">:</span>
</span><span id="line-611">                    <span class="n">counter_earlystop</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="line-612">                <span class="k">else</span><span class="p">:</span>
</span><span id="line-613">                    <span class="n">counter_earlystop</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="line-614">                <span class="k">if</span> <span class="n">counter_earlystop</span> <span class="o">&gt;=</span> <span class="n">patient_early</span><span class="p">:</span>
</span><span id="line-615">                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
</span><span id="line-616">                        <span class="sa">f</span><span class="s2">&quot;Overfitting at epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">patient_early</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="line-617">                    <span class="p">)</span>
</span><span id="line-618">                    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">train_loss</span><span class="p">[:</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">patient_early</span><span class="p">]</span>
</span><span id="line-619">                    <span class="n">val_loss</span> <span class="o">=</span> <span class="n">val_loss</span><span class="p">[:</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">patient_early</span><span class="p">]</span>
</span><span id="line-620">                    <span class="k">break</span>
</span><span id="line-621">            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
</span><span id="line-622">                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</span><span id="line-623">                <span class="n">_log_gpu_memory</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
</span><span id="line-624">                <span class="n">_log_peak_memory</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
</span><span id="line-625">
</span><span id="line-626">        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Training completed!&quot;</span><span class="p">)</span>
</span><span id="line-627">        <span class="k">return</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">val_loss</span></div>

</span><span id="line-628">
<div class="viewcode-block" id="ParticleTransformer.early_stopping">
<a class="viewcode-back" href="../../_api/pythiatransformer.transformer.html#pythiatransformer.transformer.ParticleTransformer.early_stopping">[docs]</a>
</span><span id="line-629">    <span class="k">def</span><span class="w"> </span><span class="nf">early_stopping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">current_epoch</span><span class="p">):</span>
</span><span id="line-630"><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-631"><span class="sd">        Implement early stopping.</span>
</span><span id="line-632">
</span><span id="line-633"><span class="sd">        Checks whether the validation loss has increased or remained</span>
</span><span id="line-634"><span class="sd">        the same compared to the previous epoch.</span>
</span><span id="line-635">
</span><span id="line-636"><span class="sd">        Parameters</span>
</span><span id="line-637"><span class="sd">        ----------</span>
</span><span id="line-638"><span class="sd">        val_losses: list</span>
</span><span id="line-639"><span class="sd">            List containing validation losses for each epoch before the</span>
</span><span id="line-640"><span class="sd">            current one.</span>
</span><span id="line-641"><span class="sd">        current_epoch: int</span>
</span><span id="line-642"><span class="sd">            Index of the current epoch.</span>
</span><span id="line-643"><span class="sd">        Returns</span>
</span><span id="line-644"><span class="sd">        -------</span>
</span><span id="line-645"><span class="sd">        stop: bool</span>
</span><span id="line-646"><span class="sd">            True if validation loss has no improved, False otherwise.</span>
</span><span id="line-647"><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="line-648">        <span class="k">if</span> <span class="n">val_losses</span><span class="p">[</span><span class="n">current_epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">val_losses</span><span class="p">[</span><span class="n">current_epoch</span><span class="p">]:</span>
</span><span id="line-649">            <span class="n">stop</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="line-650">        <span class="k">else</span><span class="p">:</span>
</span><span id="line-651">            <span class="n">stop</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="line-652">        <span class="k">return</span> <span class="n">stop</span></div>

</span><span id="line-653">
<div class="viewcode-block" id="ParticleTransformer.generate_targets">
<a class="viewcode-back" href="../../_api/pythiatransformer.transformer.html#pythiatransformer.transformer.ParticleTransformer.generate_targets">[docs]</a>
</span><span id="line-654">    <span class="nd">@torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">()</span>
</span><span id="line-655">    <span class="k">def</span><span class="w"> </span><span class="nf">generate_targets</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stop_threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
</span><span id="line-656"><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="line-657"><span class="sd">        Generate targets using the test set.</span>
</span><span id="line-658">
</span><span id="line-659"><span class="sd">        Autoregressively generate final particles on the test set and</span>
</span><span id="line-660"><span class="sd">        compute diagnostics.</span>
</span><span id="line-661"><span class="sd">        This runs greedy autoregressive decoding with a learned SOS</span>
</span><span id="line-662"><span class="sd">        token:</span>
</span><span id="line-663">
</span><span id="line-664"><span class="sd">        - the encoder consumes the padded status-23 inputs;</span>
</span><span id="line-665"><span class="sd">        - the decoder starts from SOS and generates one scalar `pT` at</span>
</span><span id="line-666"><span class="sd">          a time;</span>
</span><span id="line-667"><span class="sd">        - at each step an EOS probability is also predicted; if it</span>
</span><span id="line-668"><span class="sd">          exceeds ``stop_threshold`` the generation for that event</span>
</span><span id="line-669"><span class="sd">          stops.</span>
</span><span id="line-670">
</span><span id="line-671"><span class="sd">        After generation, the method computes per-event and global</span>
</span><span id="line-672"><span class="sd">        diagnostics:</span>
</span><span id="line-673">
</span><span id="line-674"><span class="sd">        - per-event relative residual on the `pT` sum;</span>
</span><span id="line-675"><span class="sd">        - per-event Wasserstein distance between the generated and</span>
</span><span id="line-676"><span class="sd">          target `pT` distributions (after removing paddings/zeros);</span>
</span><span id="line-677"><span class="sd">        - global (all-events, concatenated) Wasserstein distance and</span>
</span><span id="line-678"><span class="sd">          two-sample Kolmogorov-Smirnov statistic/p-value between</span>
</span><span id="line-679"><span class="sd">          generated and target tokens.</span>
</span><span id="line-680">
</span><span id="line-681"><span class="sd">        Parameters</span>
</span><span id="line-682"><span class="sd">        ----------</span>
</span><span id="line-683"><span class="sd">        stop_threshold : float</span>
</span><span id="line-684"><span class="sd">            Threshold on the EOS probability to stop generation for an</span>
</span><span id="line-685"><span class="sd">            event. Default is ``0.5``.</span>
</span><span id="line-686">
</span><span id="line-687"><span class="sd">        Returns</span>
</span><span id="line-688"><span class="sd">        -------</span>
</span><span id="line-689"><span class="sd">        residuals : list[float]</span>
</span><span id="line-690"><span class="sd">            Event-wise relative difference between the total `pT` of</span>
</span><span id="line-691"><span class="sd">            target and generated particles.</span>
</span><span id="line-692"><span class="sd">            For each event:</span>
</span><span id="line-693"><span class="sd">            ``(sum(target) - sum(pred)) / sum(target)``.</span>
</span><span id="line-694"><span class="sd">        wd_per_event : list[float]</span>
</span><span id="line-695"><span class="sd">            Per-event Wasserstein distances between generated and</span>
</span><span id="line-696"><span class="sd">            target tokens distributions.</span>
</span><span id="line-697"><span class="sd">        generated_tokens : numpy.ndarray</span>
</span><span id="line-698"><span class="sd">            All generated tokens `pT` from all events concatenated.</span>
</span><span id="line-699"><span class="sd">        target_tokens : numpy.ndarray</span>
</span><span id="line-700"><span class="sd">            All target tokens `pT` from all events concatenated.</span>
</span><span id="line-701"><span class="sd">        wd_global : float</span>
</span><span id="line-702"><span class="sd">            Wasserstein distance between the global generated vs target</span>
</span><span id="line-703"><span class="sd">            tokens distributions (concatenated over events).</span>
</span><span id="line-704"><span class="sd">        ks_stat : float</span>
</span><span id="line-705"><span class="sd">            Kolmogorov-Smirnov test statistic between global generated</span>
</span><span id="line-706"><span class="sd">            vs target tokens distributions.</span>
</span><span id="line-707"><span class="sd">        ks_p : float</span>
</span><span id="line-708"><span class="sd">            Kolmogorov-Smirnov two-sided p-value.</span>
</span><span id="line-709"><span class="sd">        generated_tokens_per_event : list[int]</span>
</span><span id="line-710"><span class="sd">            Number of non-padded generated tokens per event.</span>
</span><span id="line-711"><span class="sd">        target_tokens_per_event : list[int]</span>
</span><span id="line-712"><span class="sd">            Number of non-padded target tokens per event.</span>
</span><span id="line-713"><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="line-714">        <span class="c1"># Collectors.</span>
</span><span id="line-715">        <span class="n">residuals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="line-716">        <span class="n">wd_per_event</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="line-717">        <span class="n">generated_tokens</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="line-718">        <span class="n">generated_tokens_per_event</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="line-719">        <span class="n">target_tokens</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="line-720">        <span class="n">target_tokens_per_event</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="line-721">
</span><span id="line-722">        <span class="k">for</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_data</span><span class="p">:</span>
</span><span id="line-723">            <span class="nb">input</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-724">            <span class="n">target</span> <span class="o">=</span> <span class="n">target_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-725">
</span><span id="line-726">            <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="line-727">            <span class="n">max_len</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="line-728">
</span><span id="line-729">            <span class="n">enc_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="line-730">            <span class="n">dec_input</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="line-731">                <span class="bp">self</span><span class="o">.</span><span class="n">sos_token</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
</span><span id="line-732">            <span class="p">]</span>
</span><span id="line-733">            <span class="n">generated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-734">
</span><span id="line-735">            <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span><span id="line-736">                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>
</span><span id="line-737">                    <span class="n">attention_mask</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="line-738">                        <span class="n">nn</span><span class="o">.</span><span class="n">Transformer</span><span class="o">.</span><span class="n">generate_square_subsequent_mask</span><span class="p">(</span>
</span><span id="line-739">                            <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="line-740">                        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="line-741">                    <span class="p">)</span>
</span><span id="line-742">                    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span>
</span><span id="line-743">                        <span class="n">src</span><span class="o">=</span><span class="n">enc_input</span><span class="p">[</span><span class="n">event</span><span class="p">],</span>  <span class="c1"># shape (src_len, num_units).</span>
</span><span id="line-744">                        <span class="n">tgt</span><span class="o">=</span><span class="n">dec_input</span><span class="p">[</span><span class="n">event</span><span class="p">],</span>  <span class="c1"># shape (t+1, num_units).</span>
</span><span id="line-745">                        <span class="n">tgt_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>  <span class="c1"># shape (t+1, t+1).</span>
</span><span id="line-746">                    <span class="p">)</span>  <span class="c1"># shape (t+1, num_units).</span>
</span><span id="line-747">
</span><span id="line-748">                    <span class="c1"># Take the last token of the sequence.</span>
</span><span id="line-749">                    <span class="n">last_token</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="line-750">
</span><span id="line-751">                    <span class="c1"># Project it to the original feature space.</span>
</span><span id="line-752">                    <span class="n">proj_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">particle_head</span><span class="p">(</span><span class="n">last_token</span><span class="p">)</span>
</span><span id="line-753">                    <span class="n">eos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_head</span><span class="p">(</span><span class="n">last_token</span><span class="p">))</span>
</span><span id="line-754">
</span><span id="line-755">                    <span class="c1"># Save the last token in the tensor of generated</span>
</span><span id="line-756">                    <span class="c1"># tokens.</span>
</span><span id="line-757">                    <span class="n">generated</span><span class="p">[</span><span class="n">event</span><span class="p">,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">proj_token</span>
</span><span id="line-758">
</span><span id="line-759">                    <span class="c1"># Project the last token to the embedded space,</span>
</span><span id="line-760">                    <span class="c1"># then concatenate the last token/particle to</span>
</span><span id="line-761">                    <span class="c1"># dec_input for the next timestep.</span>
</span><span id="line-762">                    <span class="n">next_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span><span class="p">(</span><span class="n">proj_token</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
</span><span id="line-763">                        <span class="mi">0</span>
</span><span id="line-764">                    <span class="p">)</span>
</span><span id="line-765">                    <span class="n">dec_input</span><span class="p">[</span><span class="n">event</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
</span><span id="line-766">                        <span class="p">[</span><span class="n">dec_input</span><span class="p">[</span><span class="n">event</span><span class="p">],</span> <span class="n">next_input</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span>
</span><span id="line-767">                    <span class="p">)</span>
</span><span id="line-768">                    <span class="c1"># Verify if EOS is above the threshold.</span>
</span><span id="line-769">                    <span class="k">if</span> <span class="n">eos</span> <span class="o">&gt;</span> <span class="n">stop_threshold</span><span class="p">:</span>
</span><span id="line-770">                        <span class="k">break</span>
</span><span id="line-771">
</span><span id="line-772">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span><span id="line-773">                <span class="c1"># Compute the difference from target to predicted for</span>
</span><span id="line-774">                <span class="c1"># the pT sum in each event.</span>
</span><span id="line-775">                <span class="n">target_sum</span> <span class="o">=</span> <span class="n">target</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span><span id="line-776">                <span class="n">generated_sum</span> <span class="o">=</span> <span class="n">generated</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span><span id="line-777">                <span class="n">residuals</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">target_sum</span> <span class="o">-</span> <span class="n">generated_sum</span><span class="p">)</span> <span class="o">/</span> <span class="n">target_sum</span><span class="p">)</span>
</span><span id="line-778">
</span><span id="line-779">                <span class="c1"># Compute the wasserstain distance for each event.</span>
</span><span id="line-780">                <span class="n">generated_np</span> <span class="o">=</span> <span class="n">generated</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span><span id="line-781">                <span class="n">target_np</span> <span class="o">=</span> <span class="n">target</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span><span id="line-782">
</span><span id="line-783">                <span class="c1"># De-padding.</span>
</span><span id="line-784">                <span class="n">generated_np</span> <span class="o">=</span> <span class="n">generated_np</span><span class="p">[</span><span class="n">generated_np</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>
</span><span id="line-785">                <span class="n">target_np</span> <span class="o">=</span> <span class="n">target_np</span><span class="p">[</span><span class="n">target_np</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>
</span><span id="line-786">
</span><span id="line-787">                <span class="c1"># List of all tokens for all the events.</span>
</span><span id="line-788">                <span class="n">generated_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">generated_np</span><span class="p">)</span>
</span><span id="line-789">                <span class="n">target_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target_np</span><span class="p">)</span>
</span><span id="line-790">
</span><span id="line-791">                <span class="c1"># List of number of tokens for each event.</span>
</span><span id="line-792">                <span class="n">target_tokens_per_event</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">((</span><span class="n">target</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()))</span>
</span><span id="line-793">                <span class="n">generated_tokens_per_event</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="line-794">                    <span class="nb">int</span><span class="p">((</span><span class="n">generated</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</span><span id="line-795">                <span class="p">)</span>
</span><span id="line-796">
</span><span id="line-797">                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">generated_np</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_np</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="line-798">                    <span class="n">wd_per_event</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;nan&quot;</span><span class="p">))</span>
</span><span id="line-799">                <span class="k">else</span><span class="p">:</span>
</span><span id="line-800">                    <span class="n">wd</span> <span class="o">=</span> <span class="n">wasserstein_distance</span><span class="p">(</span><span class="n">generated_np</span><span class="p">,</span> <span class="n">target_np</span><span class="p">)</span>
</span><span id="line-801">                    <span class="n">wd_per_event</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wd</span><span class="p">)</span>
</span><span id="line-802">            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Batch completed.&quot;</span><span class="p">)</span>
</span><span id="line-803">
</span><span id="line-804">            <span class="c1"># cleanup batch</span>
</span><span id="line-805">            <span class="k">del</span> <span class="p">(</span>
</span><span id="line-806">                <span class="nb">input</span><span class="p">,</span>
</span><span id="line-807">                <span class="n">target</span><span class="p">,</span>
</span><span id="line-808">                <span class="n">enc_input</span><span class="p">,</span>
</span><span id="line-809">                <span class="n">dec_input</span><span class="p">,</span>
</span><span id="line-810">                <span class="n">generated</span><span class="p">,</span>
</span><span id="line-811">                <span class="n">output</span><span class="p">,</span>
</span><span id="line-812">                <span class="n">last_token</span><span class="p">,</span>
</span><span id="line-813">                <span class="n">eos</span><span class="p">,</span>
</span><span id="line-814">                <span class="n">proj_token</span><span class="p">,</span>
</span><span id="line-815">                <span class="n">next_input</span><span class="p">,</span>
</span><span id="line-816">                <span class="n">target_sum</span><span class="p">,</span>
</span><span id="line-817">                <span class="n">generated_sum</span><span class="p">,</span>
</span><span id="line-818">                <span class="n">generated_np</span><span class="p">,</span>
</span><span id="line-819">                <span class="n">target_np</span><span class="p">,</span>
</span><span id="line-820">                <span class="n">wd</span><span class="p">,</span>
</span><span id="line-821">            <span class="p">)</span>
</span><span id="line-822">            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</span><span id="line-823">
</span><span id="line-824">        <span class="c1"># Global distributional comparison.</span>
</span><span id="line-825">        <span class="n">generated_tokens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">generated_tokens</span><span class="p">)</span>
</span><span id="line-826">        <span class="n">target_tokens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">target_tokens</span><span class="p">)</span>
</span><span id="line-827">        <span class="n">wd_global</span> <span class="o">=</span> <span class="n">wasserstein_distance</span><span class="p">(</span><span class="n">generated_tokens</span><span class="p">,</span> <span class="n">target_tokens</span><span class="p">)</span>
</span><span id="line-828">        <span class="n">ks_stat</span><span class="p">,</span> <span class="n">ks_p</span> <span class="o">=</span> <span class="n">ks_2samp</span><span class="p">(</span><span class="n">generated_tokens</span><span class="p">,</span> <span class="n">target_tokens</span><span class="p">)</span>
</span><span id="line-829">
</span><span id="line-830">        <span class="k">return</span> <span class="p">(</span>
</span><span id="line-831">            <span class="n">residuals</span><span class="p">,</span>
</span><span id="line-832">            <span class="n">wd_per_event</span><span class="p">,</span>
</span><span id="line-833">            <span class="n">generated_tokens</span><span class="p">,</span>
</span><span id="line-834">            <span class="n">target_tokens</span><span class="p">,</span>
</span><span id="line-835">            <span class="n">wd_global</span><span class="p">,</span>
</span><span id="line-836">            <span class="n">ks_stat</span><span class="p">,</span>
</span><span id="line-837">            <span class="n">ks_p</span><span class="p">,</span>
</span><span id="line-838">            <span class="n">generated_tokens_per_event</span><span class="p">,</span>
</span><span id="line-839">            <span class="n">target_tokens_per_event</span><span class="p">,</span>
</span><span id="line-840">        <span class="p">)</span></div>
</div>

</span></code></pre></div>
    </div></div>
        </main>
      </div>
    </div><footer class="py-6 border-t border-border md:py-0">
    <div class="container flex flex-col items-center justify-between gap-4 md:h-24 md:flex-row">
      <div class="flex flex-col items-center gap-4 px-8 md:flex-row md:gap-2 md:px-0">
        <p class="text-sm leading-loose text-center text-muted-foreground md:text-left">Built with <a class="font-medium underline underline-offset-4"
    href="https://www.sphinx-doc.org"
    rel="noreferrer">Sphinx 7.4.7</a></p>
</div>
</div>
</footer>
  </div>
  
    <script src="../../_static/documentation_options.js?v=01f34227"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script defer="defer" src="../../_static/theme.js?v=073f68d9"></script>
  
</body>
</html>